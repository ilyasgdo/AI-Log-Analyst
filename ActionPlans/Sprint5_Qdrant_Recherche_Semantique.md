# Sprint 5 : Qdrant et Recherche S√©mantique - Plan d'Action D√©taill√©

## üìã Vue d'Ensemble

**Dur√©e** : 2 semaines  
**Objectif** : Impl√©menter la base de donn√©es vectorielle Qdrant et la recherche s√©mantique avanc√©e  
**√âquipe** : 1-2 d√©veloppeurs  
**Priorit√©** : √âlev√©e  
**Pr√©requis** : Sprint 4 termin√© (interface et notifications op√©rationnelles)

## üéØ Objectifs du Sprint

### Objectifs Principaux
1. **Base vectorielle Qdrant** : Installation et configuration
2. **Embeddings des logs** : G√©n√©ration et stockage des vecteurs
3. **Recherche s√©mantique** : Similarit√© et clustering des erreurs
4. **D√©tection d'anomalies** : Patterns inhabituels via ML
5. **Recommandations IA** : Suggestions bas√©es sur l'historique

### Objectifs Secondaires
- Optimisation des performances vectorielles
- Interface de recherche avanc√©e
- Visualisation des clusters d'erreurs
- Syst√®me de feedback pour am√©liorer les embeddings
- Int√©gration avec les agents existants

## üì¶ Livrables

### Infrastructure Qdrant
- [ ] Installation et configuration Qdrant
- [ ] Collections optimis√©es pour les logs
- [ ] Syst√®me de backup et restauration
- [ ] Monitoring des performances vectorielles

### Syst√®me d'Embeddings
- [ ] Pipeline de g√©n√©ration d'embeddings
- [ ] Mod√®les de vectorisation optimis√©s
- [ ] Mise √† jour incr√©mentale des vecteurs
- [ ] Cache intelligent des embeddings

### Recherche S√©mantique
- [ ] API de recherche par similarit√©
- [ ] Clustering automatique des erreurs
- [ ] D√©tection d'anomalies ML
- [ ] Syst√®me de recommandations

### Interface Avanc√©e
- [ ] Interface de recherche s√©mantique
- [ ] Visualisation des clusters
- [ ] Exploration interactive des patterns
- [ ] Dashboard d'anomalies

## üóìÔ∏è Planning D√©taill√©

### Semaine 1 : Infrastructure Qdrant et Embeddings

#### Jour 1-2 : Installation et Configuration Qdrant
**T√¢ches :**
- [ ] Installation Qdrant (Docker/local)
- [ ] Configuration des collections
- [ ] Optimisation des performances
- [ ] Tests de connectivit√©

**Configuration Qdrant :**
```python
# src/vector_db/qdrant_client.py
import asyncio
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from typing import List, Dict, Any, Optional
import numpy as np
import logging
from dataclasses import dataclass
import json

@dataclass
class QdrantConfig:
    """Configuration Qdrant"""
    host: str = "localhost"
    port: int = 6333
    api_key: Optional[str] = None
    timeout: int = 60
    
    # Collections
    logs_collection: str = "error_logs"
    diagnostics_collection: str = "diagnostics"
    patterns_collection: str = "error_patterns"
    
    # Vecteurs
    vector_size: int = 384  # all-MiniLM-L6-v2
    distance_metric: Distance = Distance.COSINE
    
    # Performance
    hnsw_config: Dict[str, Any] = None
    quantization_config: Dict[str, Any] = None

class QdrantManager:
    """Gestionnaire de la base vectorielle Qdrant"""
    
    def __init__(self, config: QdrantConfig):
        self.config = config
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialise le client Qdrant"""
        try:
            self.client = QdrantClient(
                host=self.config.host,
                port=self.config.port,
                api_key=self.config.api_key,
                timeout=self.config.timeout
            )
            logging.info("Client Qdrant initialis√©")
        except Exception as e:
            logging.error(f"Erreur initialisation Qdrant: {e}")
            raise
    
    async def setup_collections(self):
        """Configure les collections Qdrant"""
        collections_config = {
            self.config.logs_collection: {
                "description": "Logs d'erreurs avec embeddings",
                "vector_config": VectorParams(
                    size=self.config.vector_size,
                    distance=self.config.distance_metric
                ),
                "hnsw_config": models.HnswConfigDiff(
                    m=16,
                    ef_construct=100,
                    full_scan_threshold=10000
                ),
                "quantization_config": models.ScalarQuantization(
                    scalar=models.ScalarQuantizationConfig(
                        type=models.ScalarType.INT8,
                        quantile=0.99,
                        always_ram=True
                    )
                )
            },
            self.config.diagnostics_collection: {
                "description": "Diagnostics IA avec embeddings",
                "vector_config": VectorParams(
                    size=self.config.vector_size,
                    distance=self.config.distance_metric
                )
            },
            self.config.patterns_collection: {
                "description": "Patterns d'erreurs identifi√©s",
                "vector_config": VectorParams(
                    size=self.config.vector_size,
                    distance=self.config.distance_metric
                )
            }
        }
        
        for collection_name, config in collections_config.items():
            try:
                # V√©rifier si la collection existe
                collections = self.client.get_collections()
                existing_names = [col.name for col in collections.collections]
                
                if collection_name not in existing_names:
                    # Cr√©er la collection
                    self.client.create_collection(
                        collection_name=collection_name,
                        vectors_config=config["vector_config"],
                        hnsw_config=config.get("hnsw_config"),
                        quantization_config=config.get("quantization_config")
                    )
                    logging.info(f"Collection {collection_name} cr√©√©e")
                else:
                    logging.info(f"Collection {collection_name} existe d√©j√†")
                    
            except Exception as e:
                logging.error(f"Erreur cr√©ation collection {collection_name}: {e}")
                raise
    
    async def insert_log_embedding(
        self, 
        log_id: str, 
        embedding: List[float], 
        metadata: Dict[str, Any]
    ) -> bool:
        """Ins√®re un embedding de log"""
        try:
            point = PointStruct(
                id=log_id,
                vector=embedding,
                payload={
                    **metadata,
                    "timestamp": metadata.get("timestamp"),
                    "component": metadata.get("component", "unknown"),
                    "level": metadata.get("level", "INFO"),
                    "message": metadata.get("message", ""),
                    "indexed_at": datetime.utcnow().isoformat()
                }
            )
            
            self.client.upsert(
                collection_name=self.config.logs_collection,
                points=[point]
            )
            
            return True
            
        except Exception as e:
            logging.error(f"Erreur insertion embedding {log_id}: {e}")
            return False
    
    async def search_similar_logs(
        self, 
        query_embedding: List[float], 
        limit: int = 10,
        score_threshold: float = 0.7,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """Recherche des logs similaires"""
        try:
            # Construction des filtres Qdrant
            qdrant_filter = None
            if filters:
                conditions = []
                
                if "component" in filters:
                    conditions.append(
                        models.FieldCondition(
                            key="component",
                            match=models.MatchValue(value=filters["component"])
                        )
                    )
                
                if "level" in filters:
                    conditions.append(
                        models.FieldCondition(
                            key="level",
                            match=models.MatchAny(any=filters["level"])
                        )
                    )
                
                if "time_range" in filters:
                    conditions.append(
                        models.FieldCondition(
                            key="timestamp",
                            range=models.Range(
                                gte=filters["time_range"]["start"],
                                lte=filters["time_range"]["end"]
                            )
                        )
                    )
                
                if conditions:
                    qdrant_filter = models.Filter(must=conditions)
            
            # Recherche
            results = self.client.search(
                collection_name=self.config.logs_collection,
                query_vector=query_embedding,
                query_filter=qdrant_filter,
                limit=limit,
                score_threshold=score_threshold,
                with_payload=True,
                with_vectors=False
            )
            
            # Formatage des r√©sultats
            similar_logs = []
            for result in results:
                similar_logs.append({
                    "id": result.id,
                    "score": result.score,
                    "metadata": result.payload,
                    "message": result.payload.get("message", ""),
                    "component": result.payload.get("component", ""),
                    "timestamp": result.payload.get("timestamp", "")
                })
            
            return similar_logs
            
        except Exception as e:
            logging.error(f"Erreur recherche similarit√©: {e}")
            return []
    
    async def get_error_clusters(
        self, 
        min_cluster_size: int = 5,
        time_window_hours: int = 24
    ) -> List[Dict[str, Any]]:
        """Identifie les clusters d'erreurs"""
        try:
            # R√©cup√©ration des logs r√©cents
            time_filter = models.Filter(
                must=[
                    models.FieldCondition(
                        key="timestamp",
                        range=models.Range(
                            gte=(datetime.utcnow() - timedelta(hours=time_window_hours)).isoformat()
                        )
                    )
                ]
            )
            
            # Scroll pour r√©cup√©rer tous les points
            points = []
            offset = None
            
            while True:
                result = self.client.scroll(
                    collection_name=self.config.logs_collection,
                    scroll_filter=time_filter,
                    limit=1000,
                    offset=offset,
                    with_payload=True,
                    with_vectors=True
                )
                
                points.extend(result[0])
                
                if result[1] is None:  # Plus de points
                    break
                offset = result[1]
            
            if len(points) < min_cluster_size:
                return []
            
            # Clustering avec sklearn
            from sklearn.cluster import DBSCAN
            import numpy as np
            
            vectors = np.array([point.vector for point in points])
            
            # DBSCAN clustering
            clustering = DBSCAN(
                eps=0.3,  # Distance maximale entre points
                min_samples=min_cluster_size,
                metric='cosine'
            ).fit(vectors)
            
            # Analyse des clusters
            clusters = {}
            for i, (point, label) in enumerate(zip(points, clustering.labels_)):
                if label == -1:  # Bruit
                    continue
                
                if label not in clusters:
                    clusters[label] = {
                        'points': [],
                        'components': set(),
                        'levels': set(),
                        'messages': []
                    }
                
                clusters[label]['points'].append({
                    'id': point.id,
                    'payload': point.payload
                })
                clusters[label]['components'].add(point.payload.get('component', 'unknown'))
                clusters[label]['levels'].add(point.payload.get('level', 'INFO'))
                clusters[label]['messages'].append(point.payload.get('message', ''))
            
            # Formatage des r√©sultats
            cluster_results = []
            for cluster_id, cluster_data in clusters.items():
                if len(cluster_data['points']) >= min_cluster_size:
                    cluster_results.append({
                        'cluster_id': cluster_id,
                        'size': len(cluster_data['points']),
                        'components': list(cluster_data['components']),
                        'levels': list(cluster_data['levels']),
                        'sample_messages': cluster_data['messages'][:5],
                        'points': cluster_data['points']
                    })
            
            return sorted(cluster_results, key=lambda x: x['size'], reverse=True)
            
        except Exception as e:
            logging.error(f"Erreur clustering: {e}")
            return []
    
    async def detect_anomalies(
        self, 
        time_window_hours: int = 24,
        anomaly_threshold: float = 0.1
    ) -> List[Dict[str, Any]]:
        """D√©tecte les anomalies dans les patterns d'erreurs"""
        try:
            # R√©cup√©ration des logs r√©cents
            recent_logs = await self._get_recent_logs(time_window_hours)
            
            if len(recent_logs) < 10:
                return []
            
            # Analyse des patterns temporels
            from collections import defaultdict
            import pandas as pd
            
            # Groupement par heure
            hourly_counts = defaultdict(int)
            component_counts = defaultdict(int)
            level_counts = defaultdict(int)
            
            for log in recent_logs:
                timestamp = pd.to_datetime(log.payload.get('timestamp'))
                hour_key = timestamp.strftime('%Y-%m-%d %H:00:00')
                
                hourly_counts[hour_key] += 1
                component_counts[log.payload.get('component', 'unknown')] += 1
                level_counts[log.payload.get('level', 'INFO')] += 1
            
            # D√©tection d'anomalies temporelles
            anomalies = []
            
            # Anomalie: pic d'erreurs inhabituel
            counts = list(hourly_counts.values())
            if counts:
                mean_count = np.mean(counts)
                std_count = np.std(counts)
                
                for hour, count in hourly_counts.items():
                    if count > mean_count + 2 * std_count:
                        anomalies.append({
                            'type': 'temporal_spike',
                            'timestamp': hour,
                            'value': count,
                            'expected': mean_count,
                            'severity': 'high' if count > mean_count + 3 * std_count else 'medium'
                        })
            
            # Anomalie: nouveau composant avec beaucoup d'erreurs
            total_logs = len(recent_logs)
            for component, count in component_counts.items():
                if count / total_logs > anomaly_threshold:
                    # V√©rifier si c'est un nouveau pattern
                    historical_count = await self._get_historical_component_count(
                        component, 
                        time_window_hours * 7  # 7x la fen√™tre
                    )
                    
                    if historical_count == 0 or count > historical_count * 3:
                        anomalies.append({
                            'type': 'new_component_pattern',
                            'component': component,
                            'current_count': count,
                            'historical_count': historical_count,
                            'severity': 'high' if count > historical_count * 5 else 'medium'
                        })
            
            return anomalies
            
        except Exception as e:
            logging.error(f"Erreur d√©tection anomalies: {e}")
            return []
    
    async def get_recommendations(
        self, 
        error_embedding: List[float],
        context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """G√©n√®re des recommandations bas√©es sur l'historique"""
        try:
            # Recherche d'erreurs similaires r√©solues
            similar_logs = await self.search_similar_logs(
                query_embedding=error_embedding,
                limit=20,
                score_threshold=0.8,
                filters={
                    "component": context.get("component"),
                    "level": ["ERROR", "CRITICAL"]
                }
            )
            
            # Recherche de diagnostics associ√©s
            recommendations = []
            
            for similar_log in similar_logs:
                # Rechercher les diagnostics pour ce log
                diagnostic_results = self.client.search(
                    collection_name=self.config.diagnostics_collection,
                    query_vector=error_embedding,
                    query_filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="original_log_id",
                                match=models.MatchValue(value=similar_log["id"])
                            )
                        ]
                    ),
                    limit=5,
                    with_payload=True
                )
                
                for diagnostic in diagnostic_results:
                    if diagnostic.score > 0.7:
                        recommendations.append({
                            'type': 'historical_solution',
                            'similarity_score': diagnostic.score,
                            'original_error': similar_log["message"],
                            'solution': diagnostic.payload.get("solution", ""),
                            'success_rate': diagnostic.payload.get("success_rate", 0.0),
                            'last_used': diagnostic.payload.get("last_used", ""),
                            'confidence': diagnostic.payload.get("confidence", 0.5)
                        })
            
            # Tri par pertinence
            recommendations.sort(
                key=lambda x: x['similarity_score'] * x['confidence'], 
                reverse=True
            )
            
            return recommendations[:5]  # Top 5
            
        except Exception as e:
            logging.error(f"Erreur g√©n√©ration recommandations: {e}")
            return []
    
    async def health_check(self) -> Dict[str, Any]:
        """V√©rification de sant√© Qdrant"""
        try:
            # Test de connectivit√©
            collections = self.client.get_collections()
            
            # Statistiques des collections
            stats = {}
            for collection in collections.collections:
                collection_info = self.client.get_collection(collection.name)
                stats[collection.name] = {
                    'points_count': collection_info.points_count,
                    'segments_count': collection_info.segments_count,
                    'disk_data_size': collection_info.disk_data_size,
                    'ram_data_size': collection_info.ram_data_size
                }
            
            return {
                'status': 'healthy',
                'collections': len(collections.collections),
                'collection_stats': stats,
                'timestamp': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e),
                'timestamp': datetime.utcnow().isoformat()
            }
```

**Crit√®res d'acceptation :**
- Qdrant install√© et configur√©
- Collections optimis√©es cr√©√©es
- Tests de connectivit√© r√©ussis
- Monitoring des performances

#### Jour 3-4 : Syst√®me d'Embeddings
**T√¢ches :**
- [ ] Pipeline de g√©n√©ration d'embeddings
- [ ] Mod√®les de vectorisation
- [ ] Cache intelligent
- [ ] Mise √† jour incr√©mentale

**Pipeline d'Embeddings :**
```python
# src/embeddings/embedding_pipeline.py
import asyncio
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
import numpy as np
import hashlib
import json
import logging
from dataclasses import dataclass
from datetime import datetime
import redis
import pickle

@dataclass
class EmbeddingConfig:
    """Configuration des embeddings"""
    model_name: str = "all-MiniLM-L6-v2"
    max_sequence_length: int = 512
    batch_size: int = 32
    
    # Cache Redis
    redis_host: str = "localhost"
    redis_port: int = 6379
    redis_db: int = 0
    cache_ttl: int = 86400  # 24h
    
    # Optimisations
    use_gpu: bool = False
    normalize_embeddings: bool = True
    
class EmbeddingPipeline:
    """Pipeline de g√©n√©ration d'embeddings pour les logs"""
    
    def __init__(self, config: EmbeddingConfig):
        self.config = config
        self.model = None
        self.redis_client = None
        self._initialize_model()
        self._initialize_cache()
    
    def _initialize_model(self):
        """Initialise le mod√®le d'embeddings"""
        try:
            device = 'cuda' if self.config.use_gpu else 'cpu'
            self.model = SentenceTransformer(
                self.config.model_name,
                device=device
            )
            
            # Configuration du mod√®le
            self.model.max_seq_length = self.config.max_sequence_length
            
            logging.info(f"Mod√®le d'embeddings {self.config.model_name} initialis√© sur {device}")
            
        except Exception as e:
            logging.error(f"Erreur initialisation mod√®le embeddings: {e}")
            raise
    
    def _initialize_cache(self):
        """Initialise le cache Redis"""
        try:
            self.redis_client = redis.Redis(
                host=self.config.redis_host,
                port=self.config.redis_port,
                db=self.config.redis_db,
                decode_responses=False  # Pour les donn√©es binaires
            )
            
            # Test de connectivit√©
            self.redis_client.ping()
            logging.info("Cache Redis initialis√©")
            
        except Exception as e:
            logging.warning(f"Cache Redis non disponible: {e}")
            self.redis_client = None
    
    def _generate_cache_key(self, text: str) -> str:
        """G√©n√®re une cl√© de cache pour le texte"""
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        return f"embedding:{self.config.model_name}:{text_hash}"
    
    def _get_cached_embedding(self, text: str) -> Optional[np.ndarray]:
        """R√©cup√®re un embedding du cache"""
        if not self.redis_client:
            return None
        
        try:
            cache_key = self._generate_cache_key(text)
            cached_data = self.redis_client.get(cache_key)
            
            if cached_data:
                embedding = pickle.loads(cached_data)
                return np.array(embedding)
            
        except Exception as e:
            logging.warning(f"Erreur lecture cache: {e}")
        
        return None
    
    def _cache_embedding(self, text: str, embedding: np.ndarray):
        """Met en cache un embedding"""
        if not self.redis_client:
            return
        
        try:
            cache_key = self._generate_cache_key(text)
            cached_data = pickle.dumps(embedding.tolist())
            
            self.redis_client.setex(
                cache_key,
                self.config.cache_ttl,
                cached_data
            )
            
        except Exception as e:
            logging.warning(f"Erreur √©criture cache: {e}")
    
    def preprocess_log_text(self, log_data: Dict[str, Any]) -> str:
        """Pr√©processe un log pour la vectorisation"""
        # Extraction des champs pertinents
        message = log_data.get('message', '')
        component = log_data.get('component', '')
        level = log_data.get('level', '')
        
        # Construction du texte enrichi
        enriched_text = f"[{level}] {component}: {message}"
        
        # Ajout du contexte si disponible
        if 'stack_trace' in log_data:
            stack_trace = log_data['stack_trace'][:500]  # Limiter la taille
            enriched_text += f" | Stack: {stack_trace}"
        
        if 'metadata' in log_data:
            metadata = log_data['metadata']
            if isinstance(metadata, dict):
                for key, value in metadata.items():
                    if key in ['user_id', 'session_id', 'request_id']:
                        enriched_text += f" | {key}: {value}"
        
        # Nettoyage et normalisation
        enriched_text = enriched_text.replace('\n', ' ').replace('\t', ' ')
        enriched_text = ' '.join(enriched_text.split())  # Normaliser les espaces
        
        # Troncature si n√©cessaire
        if len(enriched_text) > self.config.max_sequence_length * 4:  # Approximation
            enriched_text = enriched_text[:self.config.max_sequence_length * 4]
        
        return enriched_text
    
    async def generate_embedding(self, text: str) -> np.ndarray:
        """G√©n√®re un embedding pour un texte"""
        # V√©rification du cache
        cached_embedding = self._get_cached_embedding(text)
        if cached_embedding is not None:
            return cached_embedding
        
        try:
            # G√©n√©ration de l'embedding
            embedding = self.model.encode(
                text,
                convert_to_numpy=True,
                normalize_embeddings=self.config.normalize_embeddings
            )
            
            # Mise en cache
            self._cache_embedding(text, embedding)
            
            return embedding
            
        except Exception as e:
            logging.error(f"Erreur g√©n√©ration embedding: {e}")
            # Retourner un vecteur z√©ro en cas d'erreur
            return np.zeros(self.model.get_sentence_embedding_dimension())
    
    async def generate_batch_embeddings(self, texts: List[str]) -> List[np.ndarray]:
        """G√©n√®re des embeddings par batch"""
        embeddings = []
        uncached_texts = []
        uncached_indices = []
        
        # V√©rification du cache pour chaque texte
        for i, text in enumerate(texts):
            cached_embedding = self._get_cached_embedding(text)
            if cached_embedding is not None:
                embeddings.append(cached_embedding)
            else:
                embeddings.append(None)  # Placeholder
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # G√©n√©ration des embeddings non cach√©s
        if uncached_texts:
            try:
                batch_embeddings = self.model.encode(
                    uncached_texts,
                    batch_size=self.config.batch_size,
                    convert_to_numpy=True,
                    normalize_embeddings=self.config.normalize_embeddings,
                    show_progress_bar=len(uncached_texts) > 10
                )
                
                # Mise √† jour des r√©sultats et du cache
                for i, (text, embedding) in enumerate(zip(uncached_texts, batch_embeddings)):
                    original_index = uncached_indices[i]
                    embeddings[original_index] = embedding
                    
                    # Mise en cache
                    self._cache_embedding(text, embedding)
                    
            except Exception as e:
                logging.error(f"Erreur g√©n√©ration batch embeddings: {e}")
                # Remplacer les None par des vecteurs z√©ro
                zero_vector = np.zeros(self.model.get_sentence_embedding_dimension())
                for i in uncached_indices:
                    embeddings[i] = zero_vector
        
        return embeddings
    
    async def process_log_batch(self, logs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Traite un batch de logs pour g√©n√©rer leurs embeddings"""
        # Pr√©processing
        processed_texts = [self.preprocess_log_text(log) for log in logs]
        
        # G√©n√©ration des embeddings
        embeddings = await self.generate_batch_embeddings(processed_texts)
        
        # Enrichissement des logs
        enriched_logs = []
        for log, embedding, processed_text in zip(logs, embeddings, processed_texts):
            enriched_log = {
                **log,
                'embedding': embedding.tolist(),
                'processed_text': processed_text,
                'embedding_model': self.config.model_name,
                'embedding_timestamp': datetime.utcnow().isoformat()
            }
            enriched_logs.append(enriched_log)
        
        return enriched_logs
    
    def calculate_similarity(
        self, 
        embedding1: np.ndarray, 
        embedding2: np.ndarray
    ) -> float:
        """Calcule la similarit√© cosinus entre deux embeddings"""
        try:
            # Normalisation si n√©cessaire
            if not self.config.normalize_embeddings:
                embedding1 = embedding1 / np.linalg.norm(embedding1)
                embedding2 = embedding2 / np.linalg.norm(embedding2)
            
            # Similarit√© cosinus
            similarity = np.dot(embedding1, embedding2)
            return float(similarity)
            
        except Exception as e:
            logging.error(f"Erreur calcul similarit√©: {e}")
            return 0.0
    
    def get_model_info(self) -> Dict[str, Any]:
        """Retourne les informations du mod√®le"""
        return {
            'model_name': self.config.model_name,
            'embedding_dimension': self.model.get_sentence_embedding_dimension(),
            'max_sequence_length': self.config.max_sequence_length,
            'device': str(self.model.device),
            'cache_enabled': self.redis_client is not None
        }
    
    async def cleanup_cache(self, older_than_hours: int = 168):  # 7 jours
        """Nettoie le cache des anciens embeddings"""
        if not self.redis_client:
            return
        
        try:
            # Pattern pour les cl√©s d'embeddings
            pattern = f"embedding:{self.config.model_name}:*"
            
            # Scan et suppression des anciennes cl√©s
            cursor = 0
            deleted_count = 0
            
            while True:
                cursor, keys = self.redis_client.scan(
                    cursor=cursor,
                    match=pattern,
                    count=1000
                )
                
                if keys:
                    # V√©rification de l'√¢ge des cl√©s (approximatif via TTL)
                    pipeline = self.redis_client.pipeline()
                    for key in keys:
                        ttl = self.redis_client.ttl(key)
                        if ttl < (self.config.cache_ttl - older_than_hours * 3600):
                            pipeline.delete(key)
                            deleted_count += 1
                    
                    pipeline.execute()
                
                if cursor == 0:
                    break
            
            logging.info(f"Cache nettoy√©: {deleted_count} entr√©es supprim√©es")
            
        except Exception as e:
            logging.error(f"Erreur nettoyage cache: {e}")
```

**Crit√®res d'acceptation :**
- Pipeline d'embeddings fonctionnel
- Cache Redis op√©rationnel
- Traitement par batch efficace
- Mod√®les optimis√©s charg√©s

#### Jour 5 : Tests Infrastructure
**T√¢ches :**
- [ ] Tests Qdrant et collections
- [ ] Tests pipeline embeddings
- [ ] Tests de performance
- [ ] Benchmarks de similarit√©

### Semaine 2 : Recherche S√©mantique et Interface

#### Jour 6-7 : Recherche S√©mantique Avanc√©e
**T√¢ches :**
- [ ] API de recherche par similarit√©
- [ ] Clustering automatique
- [ ] D√©tection d'anomalies ML
- [ ] Syst√®me de recommandations

#### Jour 8-9 : Interface de Recherche
**T√¢ches :**
- [ ] Interface de recherche s√©mantique
- [ ] Visualisation des clusters
- [ ] Dashboard d'anomalies
- [ ] Exploration interactive

#### Jour 10 : Tests et Optimisation
**T√¢ches :**
- [ ] Tests end-to-end recherche s√©mantique
- [ ] Optimisation des performances
- [ ] Tests de charge Qdrant
- [ ] Documentation compl√®te

## üîß Configuration Technique

### Configuration Qdrant
```yaml
# config/qdrant.yaml
qdrant:
  host: "localhost"
  port: 6333
  api_key: null
  timeout: 60
  
  collections:
    error_logs:
      vector_size: 384
      distance: "Cosine"
      hnsw_config:
        m: 16
        ef_construct: 100
        full_scan_threshold: 10000
      quantization:
        type: "int8"
        quantile: 0.99
        always_ram: true
    
    diagnostics:
      vector_size: 384
      distance: "Cosine"
    
    patterns:
      vector_size: 384
      distance: "Cosine"
  
  performance:
    max_concurrent_requests: 100
    indexing_threshold: 20000
    payload_storage_type: "on_disk"
```

### Configuration Embeddings
```yaml
# config/embeddings.yaml
embeddings:
  model_name: "all-MiniLM-L6-v2"
  max_sequence_length: 512
  batch_size: 32
  use_gpu: false
  normalize_embeddings: true
  
  cache:
    redis_host: "localhost"
    redis_port: 6379
    redis_db: 0
    ttl_seconds: 86400
  
  preprocessing:
    include_stack_trace: true
    max_stack_trace_length: 500
    include_metadata_fields:
      - "user_id"
      - "session_id"
      - "request_id"
  
  similarity:
    default_threshold: 0.7
    clustering_threshold: 0.8
    anomaly_threshold: 0.9
```

## üìä M√©triques de Succ√®s

### M√©triques Techniques
- **Temps g√©n√©ration embedding** : < 100ms
- **D√©bit Qdrant** : > 1000 req/sec
- **Pr√©cision recherche** : > 85%
- **Temps r√©ponse recherche** : < 500ms

### M√©triques Qualit√©
- **Pertinence clusters** : > 80%
- **D√©tection anomalies** : > 90%
- **Recommandations utiles** : > 75%
- **Cache hit rate** : > 60%

### M√©triques Performance
- **Utilisation m√©moire Qdrant** : < 2GB
- **Utilisation CPU embeddings** : < 50%
- **Stockage vectoriel** : Optimis√©
- **Latence r√©seau** : < 50ms

## üö® Risques et Mitigation

### Risques Techniques
| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|---------|------------|
| Performance Qdrant | Moyenne | √âlev√© | Optimisation + monitoring |
| Qualit√© embeddings | √âlev√©e | Moyen | Fine-tuning + validation |
| Consommation m√©moire | √âlev√©e | √âlev√© | Quantization + cache |

### Risques Fonctionnels
| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|---------|------------|
| Faux positifs recherche | √âlev√©e | Moyen | Seuils adaptatifs |
| Clusters non pertinents | Moyenne | Moyen | Validation humaine |

## üìù Checklist de Fin de Sprint

### Infrastructure Qdrant
- [ ] Installation et configuration r√©ussies
- [ ] Collections optimis√©es cr√©√©es
- [ ] Monitoring des performances
- [ ] Syst√®me de backup configur√©

### Syst√®me d'Embeddings
- [ ] Pipeline de g√©n√©ration fonctionnel
- [ ] Cache Redis op√©rationnel
- [ ] Mod√®les optimis√©s charg√©s
- [ ] Traitement par batch efficace

### Recherche S√©mantique
- [ ] API de recherche par similarit√©
- [ ] Clustering automatique des erreurs
- [ ] D√©tection d'anomalies ML
- [ ] Syst√®me de recommandations

### Interface Avanc√©e
- [ ] Interface de recherche s√©mantique
- [ ] Visualisation des clusters
- [ ] Dashboard d'anomalies
- [ ] Exploration interactive des patterns

### Tests et Qualit√©
- [ ] Tests unitaires > 85% coverage
- [ ] Tests de performance valid√©s
- [ ] Benchmarks de similarit√©
- [ ] Documentation compl√®te

## üéØ Crit√®res de Passage Sprint 6

Pour passer au Sprint 6, tous ces crit√®res doivent √™tre valid√©s :

1. **Qdrant op√©rationnel** : Base vectorielle performante
2. **Embeddings fonctionnels** : Pipeline de vectorisation efficace
3. **Recherche s√©mantique** : API et interface op√©rationnelles
4. **D√©tection d'anomalies** : ML et clustering fonctionnels
5. **Performance valid√©e** : M√©triques cibles atteintes

---

**üöÄ Objectif : Recherche s√©mantique intelligente et d√©tection d'anomalies ML**

*Derni√®re mise √† jour : 07/01/2025*