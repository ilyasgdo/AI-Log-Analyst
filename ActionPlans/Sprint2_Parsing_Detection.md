# Sprint 2 : Parsing et D√©tection - Plan d'Action D√©taill√©

## üìã Vue d'Ensemble

**Dur√©e** : 2 semaines  
**Objectif** : D√©velopper les capacit√©s d'ingestion, parsing et d√©tection d'erreurs  
**√âquipe** : 1-2 d√©veloppeurs  
**Priorit√©** : Critique  
**Pr√©requis** : Sprint 1 termin√© (fondations)

## üéØ Objectifs du Sprint

### Objectifs Principaux
1. **Collecteur de logs** : Ingestion temps r√©el de fichiers de logs
2. **Parser intelligent** : Extraction structur√©e des informations
3. **D√©tecteur d'erreurs** : Identification automatique des anomalies
4. **Normalisation** : Format uniforme pour tous les logs
5. **Pipeline de traitement** : Flux de donn√©es optimis√©

### Objectifs Secondaires
- Support multi-formats de logs
- Filtrage et d√©duplication
- M√©triques de performance
- Monitoring du pipeline

## üì¶ Livrables

### Code Source
- [ ] Module collector avec watchdog
- [ ] Parser multi-formats (regex + patterns)
- [ ] D√©tecteur d'erreurs configurable
- [ ] Pipeline de traitement asynchrone
- [ ] Tests unitaires et d'int√©gration

### Documentation
- [ ] Guide configuration des parsers
- [ ] Documentation patterns d'erreurs
- [ ] Guide de troubleshooting
- [ ] M√©triques et monitoring

### Configuration
- [ ] Patterns de parsing configurables
- [ ] R√®gles de d√©tection d'erreurs
- [ ] Configuration des sources
- [ ] Param√®tres de performance

## üóìÔ∏è Planning D√©taill√©

### Semaine 1 : Ingestion et Parsing

#### Jour 1-2 : Collecteur de Logs
**T√¢ches :**
- [ ] Impl√©menter FileWatcher avec watchdog
- [ ] Cr√©er syst√®me de rotation de logs
- [ ] G√©rer les reconnexions automatiques
- [ ] Impl√©menter buffer circulaire

**Architecture Collecteur :**
```python
# src/collectors/file_collector.py
import asyncio
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from typing import AsyncGenerator, Dict, Any

class LogFileCollector(FileSystemEventHandler):
    """Collecteur de logs en temps r√©el"""
    
    def __init__(self, file_path: str, buffer_size: int = 1000):
        self.file_path = file_path
        self.buffer = asyncio.Queue(maxsize=buffer_size)
        self.observer = Observer()
        self.file_position = 0
        
    async def start(self) -> None:
        """D√©marre la surveillance du fichier"""
        # Lire les logs existants
        await self._read_existing_logs()
        
        # Surveiller les nouveaux logs
        self.observer.schedule(self, path=os.path.dirname(self.file_path))
        self.observer.start()
        
    async def collect(self) -> AsyncGenerator[str, None]:
        """G√©n√©rateur asynchrone de logs"""
        while True:
            try:
                log_line = await asyncio.wait_for(
                    self.buffer.get(), timeout=1.0
                )
                yield log_line
            except asyncio.TimeoutError:
                continue
                
    def on_modified(self, event):
        """Callback watchdog pour nouveaux logs"""
        if event.src_path == self.file_path:
            asyncio.create_task(self._read_new_logs())
```

**Crit√®res d'acceptation :**
- Surveillance temps r√©el fonctionnelle
- Gestion rotation de logs
- Buffer circulaire sans perte
- Performance > 1000 logs/minute

#### Jour 3-4 : Parser Multi-Formats
**T√¢ches :**
- [ ] Cr√©er parser g√©n√©rique avec regex
- [ ] Impl√©menter parsers sp√©cialis√©s (Apache, Nginx, JSON)
- [ ] Syst√®me de d√©tection automatique de format
- [ ] Validation et normalisation des donn√©es

**Parsers Sp√©cialis√©s :**
```python
# src/parsers/log_parser.py
import re
import json
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Dict, Any, Optional

class BaseLogParser(ABC):
    """Classe de base pour tous les parsers"""
    
    @abstractmethod
    def parse(self, raw_log: str) -> Optional[Dict[str, Any]]:
        pass
    
    @abstractmethod
    def can_parse(self, raw_log: str) -> bool:
        pass

class ApacheLogParser(BaseLogParser):
    """Parser pour logs Apache/Nginx"""
    
    APACHE_PATTERN = re.compile(
        r'(?P<ip>\d+\.\d+\.\d+\.\d+) - - '
        r'\[(?P<timestamp>[^\]]+)\] '
        r'"(?P<method>\w+) (?P<path>[^\s]+) HTTP/[\d\.]+" '
        r'(?P<status>\d+) (?P<size>\d+|-)'
    )
    
    def parse(self, raw_log: str) -> Optional[Dict[str, Any]]:
        match = self.APACHE_PATTERN.match(raw_log.strip())
        if not match:
            return None
            
        return {
            'timestamp': self._parse_timestamp(match.group('timestamp')),
            'level': self._determine_level(int(match.group('status'))),
            'component': 'web_server',
            'message': f"{match.group('method')} {match.group('path')}",
            'metadata': {
                'ip': match.group('ip'),
                'method': match.group('method'),
                'path': match.group('path'),
                'status': int(match.group('status')),
                'size': match.group('size')
            },
            'raw_log': raw_log
        }

class JSONLogParser(BaseLogParser):
    """Parser pour logs JSON structur√©s"""
    
    def parse(self, raw_log: str) -> Optional[Dict[str, Any]]:
        try:
            data = json.loads(raw_log.strip())
            return {
                'timestamp': self._parse_timestamp(data.get('timestamp')),
                'level': data.get('level', 'INFO').upper(),
                'component': data.get('component', 'unknown'),
                'message': data.get('message', ''),
                'metadata': {k: v for k, v in data.items() 
                           if k not in ['timestamp', 'level', 'component', 'message']},
                'raw_log': raw_log
            }
        except json.JSONDecodeError:
            return None

class ApplicationLogParser(BaseLogParser):
    """Parser pour logs applicatifs g√©n√©riques"""
    
    APP_PATTERN = re.compile(
        r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}[,\.]\d{3}) '
        r'\[(?P<level>\w+)\] '
        r'(?P<component>[^\s]+) - '
        r'(?P<message>.*)'
    )
    
    def parse(self, raw_log: str) -> Optional[Dict[str, Any]]:
        match = self.APP_PATTERN.match(raw_log.strip())
        if not match:
            return None
            
        return {
            'timestamp': self._parse_timestamp(match.group('timestamp')),
            'level': match.group('level').upper(),
            'component': match.group('component'),
            'message': match.group('message'),
            'metadata': {},
            'raw_log': raw_log
        }
```

**Crit√®res d'acceptation :**
- Support 3+ formats de logs
- D√©tection automatique de format
- Normalisation uniforme
- Validation des donn√©es

#### Jour 5 : Tests et Int√©gration
**T√¢ches :**
- [ ] Tests unitaires pour chaque parser
- [ ] Tests d'int√©gration collecteur + parser
- [ ] Tests de performance avec gros volumes
- [ ] Validation formats de sortie

### Semaine 2 : D√©tection et Pipeline

#### Jour 6-7 : D√©tecteur d'Erreurs
**T√¢ches :**
- [ ] Cr√©er moteur de r√®gles configurable
- [ ] Impl√©menter patterns d'erreurs par cat√©gorie
- [ ] Syst√®me de scoring et priorit√©s
- [ ] D√©duplication d'erreurs similaires

**D√©tecteur d'Erreurs :**
```python
# src/detectors/error_detector.py
import re
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum

class ErrorSeverity(Enum):
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4

@dataclass
class ErrorPattern:
    name: str
    pattern: str
    severity: ErrorSeverity
    category: str
    description: str

class ErrorDetector:
    """D√©tecteur d'erreurs configurable"""
    
    def __init__(self):
        self.patterns = self._load_error_patterns()
        self.recent_errors = {}  # Pour d√©duplication
        
    def _load_error_patterns(self) -> List[ErrorPattern]:
        """Charge les patterns d'erreurs depuis la configuration"""
        return [
            # Erreurs de base de donn√©es
            ErrorPattern(
                name="db_connection_timeout",
                pattern=r"connection.*timeout|database.*timeout",
                severity=ErrorSeverity.HIGH,
                category="database",
                description="Timeout de connexion base de donn√©es"
            ),
            ErrorPattern(
                name="db_connection_refused",
                pattern=r"connection.*refused|connection.*failed",
                severity=ErrorSeverity.CRITICAL,
                category="database",
                description="Connexion base de donn√©es refus√©e"
            ),
            
            # Erreurs r√©seau
            ErrorPattern(
                name="http_5xx_error",
                pattern=r"HTTP/[\d\.]+ 5\d{2}|status.*5\d{2}",
                severity=ErrorSeverity.HIGH,
                category="network",
                description="Erreur serveur HTTP 5xx"
            ),
            ErrorPattern(
                name="api_timeout",
                pattern=r"api.*timeout|request.*timeout",
                severity=ErrorSeverity.MEDIUM,
                category="network",
                description="Timeout d'API externe"
            ),
            
            # Erreurs applicatives
            ErrorPattern(
                name="null_pointer",
                pattern=r"null.*pointer|nullpointerexception",
                severity=ErrorSeverity.MEDIUM,
                category="application",
                description="Erreur de pointeur null"
            ),
            ErrorPattern(
                name="out_of_memory",
                pattern=r"out.*of.*memory|outofmemoryerror",
                severity=ErrorSeverity.CRITICAL,
                category="infrastructure",
                description="M√©moire insuffisante"
            ),
            
            # Erreurs de s√©curit√©
            ErrorPattern(
                name="auth_failed",
                pattern=r"authentication.*failed|unauthorized",
                severity=ErrorSeverity.HIGH,
                category="security",
                description="√âchec d'authentification"
            ),
            ErrorPattern(
                name="access_denied",
                pattern=r"access.*denied|forbidden",
                severity=ErrorSeverity.MEDIUM,
                category="security",
                description="Acc√®s refus√©"
            )
        ]
    
    def detect_errors(self, parsed_log: Dict[str, Any]) -> List[Dict[str, Any]]:
        """D√©tecte les erreurs dans un log pars√©"""
        detected_errors = []
        message = parsed_log.get('message', '').lower()
        level = parsed_log.get('level', 'INFO')
        
        # V√©rification niveau de log
        if level in ['ERROR', 'FATAL', 'CRITICAL']:
            # Recherche de patterns sp√©cifiques
            for pattern in self.patterns:
                if re.search(pattern.pattern, message, re.IGNORECASE):
                    error_info = {
                        'pattern_name': pattern.name,
                        'category': pattern.category,
                        'severity': pattern.severity.value,
                        'description': pattern.description,
                        'matched_text': message,
                        'log_data': parsed_log,
                        'detected_at': datetime.utcnow().isoformat()
                    }
                    
                    # D√©duplication
                    if not self._is_duplicate(error_info):
                        detected_errors.append(error_info)
                        self._record_error(error_info)
        
        return detected_errors
    
    def _is_duplicate(self, error_info: Dict[str, Any]) -> bool:
        """V√©rifie si l'erreur est un doublon r√©cent"""
        key = f"{error_info['pattern_name']}_{error_info['category']}"
        last_seen = self.recent_errors.get(key)
        
        if last_seen:
            time_diff = datetime.utcnow() - last_seen
            return time_diff.total_seconds() < 300  # 5 minutes
        
        return False
    
    def _record_error(self, error_info: Dict[str, Any]) -> None:
        """Enregistre l'erreur pour d√©duplication"""
        key = f"{error_info['pattern_name']}_{error_info['category']}"
        self.recent_errors[key] = datetime.utcnow()
```

**Crit√®res d'acceptation :**
- D√©tection > 90% des erreurs √©videntes
- D√©duplication fonctionnelle
- Scoring par s√©v√©rit√©
- Configuration flexible

#### Jour 8-9 : Pipeline de Traitement
**T√¢ches :**
- [ ] Cr√©er pipeline asynchrone complet
- [ ] Impl√©menter syst√®me de queues
- [ ] Gestion des erreurs et retry
- [ ] Monitoring et m√©triques

**Pipeline Asynchrone :**
```python
# src/pipeline/log_pipeline.py
import asyncio
from typing import AsyncGenerator, Dict, Any, List
from dataclasses import dataclass
import logging

@dataclass
class PipelineMetrics:
    logs_processed: int = 0
    errors_detected: int = 0
    processing_time: float = 0.0
    failed_logs: int = 0

class LogProcessingPipeline:
    """Pipeline de traitement des logs"""
    
    def __init__(self, collector, parser, detector, storage):
        self.collector = collector
        self.parser = parser
        self.detector = detector
        self.storage = storage
        self.metrics = PipelineMetrics()
        self.running = False
        
    async def start(self) -> None:
        """D√©marre le pipeline de traitement"""
        self.running = True
        logging.info("D√©marrage du pipeline de traitement des logs")
        
        # D√©marrer le collecteur
        await self.collector.start()
        
        # Traitement en continu
        async for raw_log in self.collector.collect():
            if not self.running:
                break
                
            await self._process_log(raw_log)
    
    async def _process_log(self, raw_log: str) -> None:
        """Traite un log individuel"""
        start_time = asyncio.get_event_loop().time()
        
        try:
            # 1. Parsing
            parsed_log = await self._parse_log(raw_log)
            if not parsed_log:
                self.metrics.failed_logs += 1
                return
            
            # 2. Stockage du log
            await self.storage.store_log(parsed_log)
            
            # 3. D√©tection d'erreurs
            errors = self.detector.detect_errors(parsed_log)
            
            # 4. Traitement des erreurs d√©tect√©es
            for error in errors:
                await self._handle_detected_error(error)
                self.metrics.errors_detected += 1
            
            # 5. Mise √† jour m√©triques
            self.metrics.logs_processed += 1
            processing_time = asyncio.get_event_loop().time() - start_time
            self.metrics.processing_time += processing_time
            
        except Exception as e:
            logging.error(f"Erreur traitement log: {e}")
            self.metrics.failed_logs += 1
    
    async def _parse_log(self, raw_log: str) -> Dict[str, Any]:
        """Parse un log brut"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self.parser.parse, raw_log
        )
    
    async def _handle_detected_error(self, error: Dict[str, Any]) -> None:
        """Traite une erreur d√©tect√©e"""
        # Stockage de l'erreur
        await self.storage.store_error(error)
        
        # Notification si critique
        if error['severity'] >= 3:
            await self._send_alert(error)
    
    async def _send_alert(self, error: Dict[str, Any]) -> None:
        """Envoie une alerte pour erreur critique"""
        # TODO: Impl√©menter syst√®me de notification
        logging.warning(f"Erreur critique d√©tect√©e: {error['description']}")
    
    async def stop(self) -> None:
        """Arr√™te le pipeline"""
        self.running = False
        await self.collector.stop()
        logging.info("Pipeline arr√™t√©")
    
    def get_metrics(self) -> Dict[str, Any]:
        """Retourne les m√©triques du pipeline"""
        avg_processing_time = (
            self.metrics.processing_time / max(self.metrics.logs_processed, 1)
        )
        
        return {
            'logs_processed': self.metrics.logs_processed,
            'errors_detected': self.metrics.errors_detected,
            'failed_logs': self.metrics.failed_logs,
            'avg_processing_time': avg_processing_time,
            'error_rate': self.metrics.errors_detected / max(self.metrics.logs_processed, 1)
        }
```

**Crit√®res d'acceptation :**
- Pipeline asynchrone fonctionnel
- Gestion d'erreurs robuste
- M√©triques en temps r√©el
- Performance > 500 logs/minute

#### Jour 10 : Tests et Optimisation
**T√¢ches :**
- [ ] Tests de charge avec gros volumes
- [ ] Optimisation des performances
- [ ] Tests d'int√©gration compl√®te
- [ ] Documentation technique

## üîß Configuration Technique

### Configuration des Sources
```yaml
# config/sources.yaml
log_sources:
  - name: "application_logs"
    type: "file"
    path: "/var/log/app/application.log"
    format: "application"
    enabled: true
    
  - name: "nginx_access"
    type: "file"
    path: "/var/log/nginx/access.log"
    format: "apache"
    enabled: true
    
  - name: "json_logs"
    type: "file"
    path: "/var/log/app/structured.log"
    format: "json"
    enabled: true

parsing:
  buffer_size: 1000
  batch_size: 50
  timeout: 30
  retry_attempts: 3
```

### Patterns d'Erreurs
```yaml
# config/error_patterns.yaml
error_patterns:
  database:
    - name: "connection_timeout"
      pattern: "connection.*timeout|database.*timeout"
      severity: "high"
      
    - name: "deadlock"
      pattern: "deadlock.*detected|lock.*timeout"
      severity: "medium"
      
  network:
    - name: "http_5xx"
      pattern: "HTTP/[\d\.]+ 5\d{2}|status.*5\d{2}"
      severity: "high"
      
    - name: "api_timeout"
      pattern: "api.*timeout|request.*timeout"
      severity: "medium"

detection:
  deduplication_window: 300  # 5 minutes
  min_severity: 1
  max_errors_per_minute: 100
```

## üìä M√©triques de Succ√®s

### M√©triques Techniques
- **D√©bit** : > 500 logs/minute trait√©s
- **Latence** : < 100ms par log
- **Pr√©cision d√©tection** : > 90%
- **Faux positifs** : < 5%

### M√©triques Qualit√©
- **Coverage tests** : > 85%
- **Performance** : Stable sur 8h
- **M√©moire** : < 200MB utilisation
- **CPU** : < 20% utilisation moyenne

### M√©triques Fonctionnelles
- **Formats support√©s** : 3+ types de logs
- **Patterns d'erreurs** : 20+ patterns configur√©s
- **D√©duplication** : Fonctionnelle sur 5 minutes
- **Pipeline** : Traitement temps r√©el stable

## üö® Risques et Mitigation

### Risques Techniques
| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|---------|------------|
| Performance parsing | Moyenne | √âlev√© | Optimisation regex + tests charge |
| Perte de logs | Faible | Critique | Buffer persistant + monitoring |
| Faux positifs | √âlev√©e | Moyen | Fine-tuning patterns + validation |

### Risques Fonctionnels
| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|---------|------------|
| Formats non support√©s | Moyenne | Moyen | Parser g√©n√©rique + extensibilit√© |
| Volume trop √©lev√© | Faible | √âlev√© | Syst√®me de queues + scaling |

## üìù Checklist de Fin de Sprint

### Code
- [ ] Collecteur temps r√©el fonctionnel
- [ ] Parsers multi-formats impl√©ment√©s
- [ ] D√©tecteur d'erreurs configurable
- [ ] Pipeline asynchrone op√©rationnel
- [ ] Tests unitaires et d'int√©gration

### Performance
- [ ] D√©bit > 500 logs/minute
- [ ] Latence < 100ms par log
- [ ] Stabilit√© sur 8h continues
- [ ] Utilisation ressources optimis√©e

### Configuration
- [ ] Patterns d'erreurs configurables
- [ ] Sources multiples support√©es
- [ ] Param√®tres de performance ajustables
- [ ] Monitoring et m√©triques

### Documentation
- [ ] Guide configuration parsers
- [ ] Documentation patterns d'erreurs
- [ ] Guide troubleshooting
- [ ] M√©triques et alertes

## üéØ Crit√®res de Passage Sprint 3

Pour passer au Sprint 3, tous ces crit√®res doivent √™tre valid√©s :

1. **Ingestion** : Collecteur temps r√©el stable et performant
2. **Parsing** : Support multi-formats avec normalisation
3. **D√©tection** : Identification automatique d'erreurs > 90%
4. **Pipeline** : Traitement asynchrone fonctionnel
5. **Tests** : Coverage > 85% et tests de charge OK

---

**üöÄ Objectif : Pipeline de traitement des logs robuste et performant**

*Derni√®re mise √† jour : 07/01/2025*