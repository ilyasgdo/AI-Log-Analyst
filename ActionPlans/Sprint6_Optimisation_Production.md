# Sprint 6 : Optimisation et Production - Plan d'Action D√©taill√©

## üìã Vue d'Ensemble

**Dur√©e** : 2 semaines  
**Objectif** : Optimiser les performances et pr√©parer le d√©ploiement en production  
**√âquipe** : 1-2 d√©veloppeurs  
**Priorit√©** : Critique  
**Pr√©requis** : Sprint 5 termin√© (Qdrant et recherche s√©mantique op√©rationnels)

## üéØ Objectifs du Sprint

### Objectifs Principaux
1. **Optimisation des performances** : Profiling et am√©lioration des goulots d'√©tranglement
2. **D√©ploiement production** : Containerisation et orchestration
3. **Monitoring avanc√©** : M√©triques, alertes et observabilit√©
4. **S√©curit√© renforc√©e** : Authentification, autorisation et chiffrement
5. **Documentation compl√®te** : Guide d'installation, API et maintenance

### Objectifs Secondaires
- Syst√®me de backup automatis√©
- Haute disponibilit√© et failover
- Tests de charge et stress
- Optimisation des co√ªts d'infrastructure
- Int√©gration CI/CD compl√®te

## üì¶ Livrables

### Optimisation Performance
- [ ] Profiling complet de l'application
- [ ] Optimisation des requ√™tes Qdrant
- [ ] Cache multi-niveaux optimis√©
- [ ] Compression et s√©rialisation efficace

### Infrastructure Production
- [ ] Images Docker optimis√©es
- [ ] Configuration Kubernetes/Docker Compose
- [ ] Syst√®me de backup automatis√©
- [ ] Monitoring et alerting complets

### S√©curit√©
- [ ] Authentification JWT/OAuth2
- [ ] Chiffrement des donn√©es sensibles
- [ ] Audit et logging s√©curis√©
- [ ] Scan de vuln√©rabilit√©s

### Documentation
- [ ] Guide d'installation production
- [ ] Documentation API compl√®te
- [ ] Runbooks de maintenance
- [ ] Guide de troubleshooting

## üóìÔ∏è Planning D√©taill√©

### Semaine 1 : Optimisation et Performance

#### Jour 1-2 : Profiling et Optimisation
**T√¢ches :**
- [ ] Profiling complet de l'application
- [ ] Identification des goulots d'√©tranglement
- [ ] Optimisation des requ√™tes critiques
- [ ] Am√©lioration du cache

**Syst√®me de Profiling :**
```python
# src/monitoring/performance_profiler.py
import asyncio
import time
import psutil
import logging
from typing import Dict, Any, List, Optional, Callable
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import json
import threading
from contextlib import asynccontextmanager
import cProfile
import pstats
import io
from functools import wraps
import tracemalloc
import gc

@dataclass
class PerformanceMetrics:
    """M√©triques de performance"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    disk_io_read_mb: float
    disk_io_write_mb: float
    network_sent_mb: float
    network_recv_mb: float
    active_threads: int
    open_files: int

@dataclass
class FunctionMetrics:
    """M√©triques d'une fonction"""
    function_name: str
    call_count: int
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    memory_peak_mb: float
    last_called: str

class PerformanceProfiler:
    """Profileur de performance pour l'application"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.metrics_history: List[PerformanceMetrics] = []
        self.function_metrics: Dict[str, FunctionMetrics] = {}
        self.is_monitoring = False
        self.monitoring_thread = None
        
        # Configuration
        self.monitoring_interval = config.get('monitoring_interval', 5)  # secondes
        self.max_history_size = config.get('max_history_size', 1000)
        self.enable_memory_profiling = config.get('enable_memory_profiling', True)
        
        # Initialisation du monitoring m√©moire
        if self.enable_memory_profiling:
            tracemalloc.start()
    
    def start_monitoring(self):
        """D√©marre le monitoring syst√®me"""
        if self.is_monitoring:
            return
        
        self.is_monitoring = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            daemon=True
        )
        self.monitoring_thread.start()
        logging.info("Monitoring de performance d√©marr√©")
    
    def stop_monitoring(self):
        """Arr√™te le monitoring syst√®me"""
        self.is_monitoring = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        logging.info("Monitoring de performance arr√™t√©")
    
    def _monitoring_loop(self):
        """Boucle de monitoring syst√®me"""
        last_disk_io = psutil.disk_io_counters()
        last_network_io = psutil.net_io_counters()
        
        while self.is_monitoring:
            try:
                # M√©triques syst√®me
                cpu_percent = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                
                # I/O disque
                current_disk_io = psutil.disk_io_counters()
                disk_read_mb = (current_disk_io.read_bytes - last_disk_io.read_bytes) / 1024 / 1024
                disk_write_mb = (current_disk_io.write_bytes - last_disk_io.write_bytes) / 1024 / 1024
                last_disk_io = current_disk_io
                
                # I/O r√©seau
                current_network_io = psutil.net_io_counters()
                network_sent_mb = (current_network_io.bytes_sent - last_network_io.bytes_sent) / 1024 / 1024
                network_recv_mb = (current_network_io.bytes_recv - last_network_io.bytes_recv) / 1024 / 1024
                last_network_io = current_network_io
                
                # Processus actuel
                process = psutil.Process()
                
                metrics = PerformanceMetrics(
                    timestamp=datetime.utcnow().isoformat(),
                    cpu_percent=cpu_percent,
                    memory_percent=memory.percent,
                    memory_used_mb=memory.used / 1024 / 1024,
                    disk_io_read_mb=disk_read_mb,
                    disk_io_write_mb=disk_write_mb,
                    network_sent_mb=network_sent_mb,
                    network_recv_mb=network_recv_mb,
                    active_threads=threading.active_count(),
                    open_files=len(process.open_files())
                )
                
                # Stockage des m√©triques
                self.metrics_history.append(metrics)
                
                # Limitation de l'historique
                if len(self.metrics_history) > self.max_history_size:
                    self.metrics_history = self.metrics_history[-self.max_history_size:]
                
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                logging.error(f"Erreur monitoring: {e}")
                time.sleep(self.monitoring_interval)
    
    def profile_function(self, func_name: Optional[str] = None):
        """D√©corateur pour profiler une fonction"""
        def decorator(func: Callable):
            function_name = func_name or f"{func.__module__}.{func.__name__}"
            
            @wraps(func)
            async def async_wrapper(*args, **kwargs):
                return await self._profile_async_function(function_name, func, *args, **kwargs)
            
            @wraps(func)
            def sync_wrapper(*args, **kwargs):
                return self._profile_sync_function(function_name, func, *args, **kwargs)
            
            return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
        
        return decorator
    
    async def _profile_async_function(self, function_name: str, func: Callable, *args, **kwargs):
        """Profile une fonction asynchrone"""
        start_time = time.time()
        start_memory = 0
        
        if self.enable_memory_profiling:
            gc.collect()  # Force garbage collection
            start_memory = tracemalloc.get_traced_memory()[0] / 1024 / 1024
        
        try:
            result = await func(*args, **kwargs)
            success = True
        except Exception as e:
            success = False
            raise
        finally:
            end_time = time.time()
            execution_time = end_time - start_time
            
            end_memory = 0
            if self.enable_memory_profiling:
                end_memory = tracemalloc.get_traced_memory()[0] / 1024 / 1024
            
            memory_used = max(0, end_memory - start_memory)
            
            self._update_function_metrics(function_name, execution_time, memory_used)
        
        return result
    
    def _profile_sync_function(self, function_name: str, func: Callable, *args, **kwargs):
        """Profile une fonction synchrone"""
        start_time = time.time()
        start_memory = 0
        
        if self.enable_memory_profiling:
            gc.collect()
            start_memory = tracemalloc.get_traced_memory()[0] / 1024 / 1024
        
        try:
            result = func(*args, **kwargs)
            success = True
        except Exception as e:
            success = False
            raise
        finally:
            end_time = time.time()
            execution_time = end_time - start_time
            
            end_memory = 0
            if self.enable_memory_profiling:
                end_memory = tracemalloc.get_traced_memory()[0] / 1024 / 1024
            
            memory_used = max(0, end_memory - start_memory)
            
            self._update_function_metrics(function_name, execution_time, memory_used)
        
        return result
    
    def _update_function_metrics(self, function_name: str, execution_time: float, memory_used: float):
        """Met √† jour les m√©triques d'une fonction"""
        if function_name not in self.function_metrics:
            self.function_metrics[function_name] = FunctionMetrics(
                function_name=function_name,
                call_count=0,
                total_time=0.0,
                avg_time=0.0,
                min_time=float('inf'),
                max_time=0.0,
                memory_peak_mb=0.0,
                last_called=""
            )
        
        metrics = self.function_metrics[function_name]
        metrics.call_count += 1
        metrics.total_time += execution_time
        metrics.avg_time = metrics.total_time / metrics.call_count
        metrics.min_time = min(metrics.min_time, execution_time)
        metrics.max_time = max(metrics.max_time, execution_time)
        metrics.memory_peak_mb = max(metrics.memory_peak_mb, memory_used)
        metrics.last_called = datetime.utcnow().isoformat()
    
    @asynccontextmanager
    async def profile_block(self, block_name: str):
        """Context manager pour profiler un bloc de code"""
        start_time = time.time()
        start_memory = 0
        
        if self.enable_memory_profiling:
            gc.collect()
            start_memory = tracemalloc.get_traced_memory()[0] / 1024 / 1024
        
        try:
            yield
        finally:
            end_time = time.time()
            execution_time = end_time - start_time
            
            end_memory = 0
            if self.enable_memory_profiling:
                end_memory = tracemalloc.get_traced_memory()[0] / 1024 / 1024
            
            memory_used = max(0, end_memory - start_memory)
            
            self._update_function_metrics(f"block:{block_name}", execution_time, memory_used)
    
    def get_performance_report(self, last_minutes: int = 60) -> Dict[str, Any]:
        """G√©n√®re un rapport de performance"""
        cutoff_time = datetime.utcnow() - timedelta(minutes=last_minutes)
        
        # Filtrage des m√©triques r√©centes
        recent_metrics = [
            m for m in self.metrics_history
            if datetime.fromisoformat(m.timestamp) > cutoff_time
        ]
        
        if not recent_metrics:
            return {"error": "Aucune m√©trique disponible"}
        
        # Calculs statistiques
        cpu_values = [m.cpu_percent for m in recent_metrics]
        memory_values = [m.memory_percent for m in recent_metrics]
        
        # Top fonctions par temps total
        top_functions_by_time = sorted(
            self.function_metrics.values(),
            key=lambda x: x.total_time,
            reverse=True
        )[:10]
        
        # Top fonctions par temps moyen
        top_functions_by_avg = sorted(
            self.function_metrics.values(),
            key=lambda x: x.avg_time,
            reverse=True
        )[:10]
        
        # Top fonctions par m√©moire
        top_functions_by_memory = sorted(
            self.function_metrics.values(),
            key=lambda x: x.memory_peak_mb,
            reverse=True
        )[:10]
        
        return {
            "report_period_minutes": last_minutes,
            "system_metrics": {
                "cpu": {
                    "avg": sum(cpu_values) / len(cpu_values),
                    "min": min(cpu_values),
                    "max": max(cpu_values),
                    "current": recent_metrics[-1].cpu_percent
                },
                "memory": {
                    "avg": sum(memory_values) / len(memory_values),
                    "min": min(memory_values),
                    "max": max(memory_values),
                    "current": recent_metrics[-1].memory_percent
                },
                "threads": recent_metrics[-1].active_threads,
                "open_files": recent_metrics[-1].open_files
            },
            "function_metrics": {
                "total_functions_profiled": len(self.function_metrics),
                "top_by_total_time": [asdict(f) for f in top_functions_by_time],
                "top_by_avg_time": [asdict(f) for f in top_functions_by_avg],
                "top_by_memory": [asdict(f) for f in top_functions_by_memory]
            },
            "recommendations": self._generate_recommendations(recent_metrics)
        }
    
    def _generate_recommendations(self, metrics: List[PerformanceMetrics]) -> List[str]:
        """G√©n√®re des recommandations d'optimisation"""
        recommendations = []
        
        if not metrics:
            return recommendations
        
        # Analyse CPU
        avg_cpu = sum(m.cpu_percent for m in metrics) / len(metrics)
        if avg_cpu > 80:
            recommendations.append("CPU √©lev√©: Consid√©rer l'optimisation des algorithmes ou l'ajout de ressources")
        
        # Analyse m√©moire
        avg_memory = sum(m.memory_percent for m in metrics) / len(metrics)
        if avg_memory > 85:
            recommendations.append("M√©moire √©lev√©e: V√©rifier les fuites m√©moire et optimiser le cache")
        
        # Analyse des fonctions lentes
        slow_functions = [
            f for f in self.function_metrics.values()
            if f.avg_time > 1.0  # Plus d'1 seconde en moyenne
        ]
        if slow_functions:
            recommendations.append(f"Fonctions lentes d√©tect√©es: {len(slow_functions)} fonctions > 1s")
        
        # Analyse m√©moire des fonctions
        memory_heavy_functions = [
            f for f in self.function_metrics.values()
            if f.memory_peak_mb > 100  # Plus de 100MB
        ]
        if memory_heavy_functions:
            recommendations.append(f"Fonctions gourmandes en m√©moire: {len(memory_heavy_functions)} fonctions > 100MB")
        
        # Analyse des threads
        if metrics[-1].active_threads > 50:
            recommendations.append("Nombre √©lev√© de threads: V√©rifier les pools de threads")
        
        return recommendations
    
    def export_metrics(self, filepath: str, format: str = "json"):
        """Exporte les m√©triques vers un fichier"""
        try:
            data = {
                "export_timestamp": datetime.utcnow().isoformat(),
                "system_metrics": [asdict(m) for m in self.metrics_history],
                "function_metrics": [asdict(f) for f in self.function_metrics.values()],
                "config": self.config
            }
            
            if format.lower() == "json":
                with open(filepath, 'w') as f:
                    json.dump(data, f, indent=2)
            else:
                raise ValueError(f"Format non support√©: {format}")
            
            logging.info(f"M√©triques export√©es vers {filepath}")
            
        except Exception as e:
            logging.error(f"Erreur export m√©triques: {e}")
    
    def reset_metrics(self):
        """Remet √† z√©ro toutes les m√©triques"""
        self.metrics_history.clear()
        self.function_metrics.clear()
        logging.info("M√©triques remises √† z√©ro")

# Utilisation globale du profileur
profiler = None

def initialize_profiler(config: Dict[str, Any]):
    """Initialise le profileur global"""
    global profiler
    profiler = PerformanceProfiler(config)
    profiler.start_monitoring()
    return profiler

def get_profiler() -> Optional[PerformanceProfiler]:
    """Retourne le profileur global"""
    return profiler

# D√©corateurs de convenance
def profile(func_name: Optional[str] = None):
    """D√©corateur pour profiler une fonction"""
    if profiler:
        return profiler.profile_function(func_name)
    else:
        # D√©corateur no-op si pas de profileur
        def decorator(func):
            return func
        return decorator
```

**Optimisation Cache Multi-Niveaux :**
```python
# src/cache/multi_level_cache.py
import asyncio
import redis
import json
import pickle
import hashlib
import logging
from typing import Any, Optional, Dict, List, Union
from dataclasses import dataclass
from datetime import datetime, timedelta
import threading
import time
from abc import ABC, abstractmethod

@dataclass
class CacheConfig:
    """Configuration du cache multi-niveaux"""
    # Cache L1 (m√©moire locale)
    l1_max_size: int = 1000
    l1_ttl_seconds: int = 300  # 5 minutes
    
    # Cache L2 (Redis)
    l2_host: str = "localhost"
    l2_port: int = 6379
    l2_db: int = 1
    l2_ttl_seconds: int = 3600  # 1 heure
    
    # Cache L3 (disque local)
    l3_enabled: bool = True
    l3_directory: str = "./cache"
    l3_max_size_mb: int = 500
    l3_ttl_seconds: int = 86400  # 24 heures
    
    # Compression
    enable_compression: bool = True
    compression_threshold: int = 1024  # bytes
    
    # Monitoring
    enable_metrics: bool = True

@dataclass
class CacheEntry:
    """Entr√©e de cache"""
    key: str
    value: Any
    timestamp: datetime
    ttl_seconds: int
    access_count: int = 0
    last_access: Optional[datetime] = None
    size_bytes: int = 0

class CacheLevel(ABC):
    """Interface pour un niveau de cache"""
    
    @abstractmethod
    async def get(self, key: str) -> Optional[Any]:
        pass
    
    @abstractmethod
    async def set(self, key: str, value: Any, ttl_seconds: int) -> bool:
        pass
    
    @abstractmethod
    async def delete(self, key: str) -> bool:
        pass
    
    @abstractmethod
    async def clear(self) -> bool:
        pass
    
    @abstractmethod
    def get_stats(self) -> Dict[str, Any]:
        pass

class L1MemoryCache(CacheLevel):
    """Cache L1 - M√©moire locale avec LRU"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.cache: Dict[str, CacheEntry] = {}
        self.access_order: List[str] = []
        self.lock = threading.RLock()
        
        # M√©triques
        self.hits = 0
        self.misses = 0
        self.evictions = 0
    
    async def get(self, key: str) -> Optional[Any]:
        with self.lock:
            if key not in self.cache:
                self.misses += 1
                return None
            
            entry = self.cache[key]
            
            # V√©rification TTL
            if self._is_expired(entry):
                del self.cache[key]
                if key in self.access_order:
                    self.access_order.remove(key)
                self.misses += 1
                return None
            
            # Mise √† jour des statistiques d'acc√®s
            entry.access_count += 1
            entry.last_access = datetime.utcnow()
            
            # Mise √† jour de l'ordre LRU
            if key in self.access_order:
                self.access_order.remove(key)
            self.access_order.append(key)
            
            self.hits += 1
            return entry.value
    
    async def set(self, key: str, value: Any, ttl_seconds: int) -> bool:
        with self.lock:
            try:
                # Calcul de la taille approximative
                size_bytes = len(str(value).encode('utf-8'))
                
                entry = CacheEntry(
                    key=key,
                    value=value,
                    timestamp=datetime.utcnow(),
                    ttl_seconds=ttl_seconds,
                    size_bytes=size_bytes
                )
                
                # √âviction si n√©cessaire
                await self._evict_if_needed()
                
                # Ajout/mise √† jour
                if key in self.cache:
                    if key in self.access_order:
                        self.access_order.remove(key)
                
                self.cache[key] = entry
                self.access_order.append(key)
                
                return True
                
            except Exception as e:
                logging.error(f"Erreur L1 cache set: {e}")
                return False
    
    async def delete(self, key: str) -> bool:
        with self.lock:
            if key in self.cache:
                del self.cache[key]
                if key in self.access_order:
                    self.access_order.remove(key)
                return True
            return False
    
    async def clear(self) -> bool:
        with self.lock:
            self.cache.clear()
            self.access_order.clear()
            return True
    
    def _is_expired(self, entry: CacheEntry) -> bool:
        """V√©rifie si une entr√©e a expir√©"""
        age = datetime.utcnow() - entry.timestamp
        return age.total_seconds() > entry.ttl_seconds
    
    async def _evict_if_needed(self):
        """√âviction LRU si n√©cessaire"""
        while len(self.cache) >= self.config.l1_max_size:
            if not self.access_order:
                break
            
            # √âviction du plus ancien
            oldest_key = self.access_order.pop(0)
            if oldest_key in self.cache:
                del self.cache[oldest_key]
                self.evictions += 1
    
    def get_stats(self) -> Dict[str, Any]:
        with self.lock:
            total_requests = self.hits + self.misses
            hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0
            
            return {
                "level": "L1_Memory",
                "entries": len(self.cache),
                "max_entries": self.config.l1_max_size,
                "hits": self.hits,
                "misses": self.misses,
                "hit_rate_percent": round(hit_rate, 2),
                "evictions": self.evictions,
                "total_size_bytes": sum(entry.size_bytes for entry in self.cache.values())
            }

class L2RedisCache(CacheLevel):
    """Cache L2 - Redis distribu√©"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.redis_client = None
        self._initialize_redis()
        
        # M√©triques
        self.hits = 0
        self.misses = 0
        self.errors = 0
    
    def _initialize_redis(self):
        """Initialise la connexion Redis"""
        try:
            self.redis_client = redis.Redis(
                host=self.config.l2_host,
                port=self.config.l2_port,
                db=self.config.l2_db,
                decode_responses=False,
                socket_timeout=5,
                socket_connect_timeout=5
            )
            
            # Test de connectivit√©
            self.redis_client.ping()
            logging.info("Cache L2 Redis initialis√©")
            
        except Exception as e:
            logging.error(f"Erreur initialisation Redis: {e}")
            self.redis_client = None
    
    async def get(self, key: str) -> Optional[Any]:
        if not self.redis_client:
            self.errors += 1
            return None
        
        try:
            cache_key = self._generate_key(key)
            data = self.redis_client.get(cache_key)
            
            if data is None:
                self.misses += 1
                return None
            
            # D√©s√©rialisation
            value = self._deserialize(data)
            self.hits += 1
            return value
            
        except Exception as e:
            logging.error(f"Erreur L2 cache get: {e}")
            self.errors += 1
            return None
    
    async def set(self, key: str, value: Any, ttl_seconds: int) -> bool:
        if not self.redis_client:
            self.errors += 1
            return False
        
        try:
            cache_key = self._generate_key(key)
            serialized_data = self._serialize(value)
            
            # Stockage avec TTL
            self.redis_client.setex(
                cache_key,
                ttl_seconds,
                serialized_data
            )
            
            return True
            
        except Exception as e:
            logging.error(f"Erreur L2 cache set: {e}")
            self.errors += 1
            return False
    
    async def delete(self, key: str) -> bool:
        if not self.redis_client:
            return False
        
        try:
            cache_key = self._generate_key(key)
            result = self.redis_client.delete(cache_key)
            return result > 0
            
        except Exception as e:
            logging.error(f"Erreur L2 cache delete: {e}")
            return False
    
    async def clear(self) -> bool:
        if not self.redis_client:
            return False
        
        try:
            # Suppression par pattern
            pattern = f"ai_log_cache:*"
            keys = self.redis_client.keys(pattern)
            
            if keys:
                self.redis_client.delete(*keys)
            
            return True
            
        except Exception as e:
            logging.error(f"Erreur L2 cache clear: {e}")
            return False
    
    def _generate_key(self, key: str) -> str:
        """G√©n√®re une cl√© Redis avec pr√©fixe"""
        return f"ai_log_cache:{key}"
    
    def _serialize(self, value: Any) -> bytes:
        """S√©rialise une valeur pour Redis"""
        try:
            # Compression si activ√©e et si la valeur est assez grande
            data = pickle.dumps(value)
            
            if (self.config.enable_compression and 
                len(data) > self.config.compression_threshold):
                import gzip
                data = gzip.compress(data)
                # Marqueur de compression
                data = b'COMPRESSED:' + data
            
            return data
            
        except Exception as e:
            logging.error(f"Erreur s√©rialisation: {e}")
            raise
    
    def _deserialize(self, data: bytes) -> Any:
        """D√©s√©rialise une valeur depuis Redis"""
        try:
            # V√©rification de la compression
            if data.startswith(b'COMPRESSED:'):
                import gzip
                data = gzip.decompress(data[11:])  # Enlever le marqueur
            
            return pickle.loads(data)
            
        except Exception as e:
            logging.error(f"Erreur d√©s√©rialisation: {e}")
            raise
    
    def get_stats(self) -> Dict[str, Any]:
        total_requests = self.hits + self.misses
        hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0
        
        redis_info = {}
        if self.redis_client:
            try:
                info = self.redis_client.info()
                redis_info = {
                    "used_memory_mb": info.get('used_memory', 0) / 1024 / 1024,
                    "connected_clients": info.get('connected_clients', 0),
                    "keyspace_hits": info.get('keyspace_hits', 0),
                    "keyspace_misses": info.get('keyspace_misses', 0)
                }
            except:
                pass
        
        return {
            "level": "L2_Redis",
            "available": self.redis_client is not None,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate_percent": round(hit_rate, 2),
            "errors": self.errors,
            "redis_info": redis_info
        }

class MultiLevelCache:
    """Cache multi-niveaux intelligent"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        
        # Initialisation des niveaux
        self.l1_cache = L1MemoryCache(config)
        self.l2_cache = L2RedisCache(config)
        
        # M√©triques globales
        self.total_requests = 0
        self.l1_hits = 0
        self.l2_hits = 0
        self.total_misses = 0
        
        logging.info("Cache multi-niveaux initialis√©")
    
    async def get(self, key: str) -> Optional[Any]:
        """R√©cup√®re une valeur du cache (L1 -> L2 -> None)"""
        self.total_requests += 1
        
        # Tentative L1
        value = await self.l1_cache.get(key)
        if value is not None:
            self.l1_hits += 1
            return value
        
        # Tentative L2
        value = await self.l2_cache.get(key)
        if value is not None:
            self.l2_hits += 1
            
            # Promotion vers L1
            await self.l1_cache.set(key, value, self.config.l1_ttl_seconds)
            
            return value
        
        # Cache miss complet
        self.total_misses += 1
        return None
    
    async def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> bool:
        """Stocke une valeur dans tous les niveaux de cache"""
        if ttl_seconds is None:
            ttl_seconds = self.config.l1_ttl_seconds
        
        success = True
        
        # Stockage L1
        l1_success = await self.l1_cache.set(key, value, ttl_seconds)
        success = success and l1_success
        
        # Stockage L2 avec TTL plus long
        l2_ttl = max(ttl_seconds, self.config.l2_ttl_seconds)
        l2_success = await self.l2_cache.set(key, value, l2_ttl)
        success = success and l2_success
        
        return success
    
    async def delete(self, key: str) -> bool:
        """Supprime une valeur de tous les niveaux"""
        l1_success = await self.l1_cache.delete(key)
        l2_success = await self.l2_cache.delete(key)
        
        return l1_success or l2_success
    
    async def clear(self) -> bool:
        """Vide tous les niveaux de cache"""
        l1_success = await self.l1_cache.clear()
        l2_success = await self.l2_cache.clear()
        
        # Reset des m√©triques
        self.total_requests = 0
        self.l1_hits = 0
        self.l2_hits = 0
        self.total_misses = 0
        
        return l1_success and l2_success
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques compl√®tes du cache"""
        l1_stats = self.l1_cache.get_stats()
        l2_stats = self.l2_cache.get_stats()
        
        # Calculs globaux
        total_hit_rate = 0
        if self.total_requests > 0:
            total_hits = self.l1_hits + self.l2_hits
            total_hit_rate = (total_hits / self.total_requests) * 100
        
        return {
            "global_stats": {
                "total_requests": self.total_requests,
                "l1_hits": self.l1_hits,
                "l2_hits": self.l2_hits,
                "total_misses": self.total_misses,
                "total_hit_rate_percent": round(total_hit_rate, 2),
                "l1_hit_rate_percent": round((self.l1_hits / self.total_requests * 100) if self.total_requests > 0 else 0, 2),
                "l2_hit_rate_percent": round((self.l2_hits / self.total_requests * 100) if self.total_requests > 0 else 0, 2)
            },
            "l1_stats": l1_stats,
            "l2_stats": l2_stats,
            "config": {
                "l1_max_size": self.config.l1_max_size,
                "l1_ttl_seconds": self.config.l1_ttl_seconds,
                "l2_ttl_seconds": self.config.l2_ttl_seconds,
                "compression_enabled": self.config.enable_compression
            }
        }
    
    async def warm_up(self, keys_values: Dict[str, Any]):
        """Pr√©chauffe le cache avec des donn√©es"""
        logging.info(f"Pr√©chauffage du cache avec {len(keys_values)} entr√©es")
        
        for key, value in keys_values.items():
            await self.set(key, value)
        
        logging.info("Pr√©chauffage du cache termin√©")
    
    async def health_check(self) -> Dict[str, Any]:
        """V√©rification de sant√© du cache"""
        health = {
            "status": "healthy",
            "levels": {
                "l1": {"status": "healthy", "available": True},
                "l2": {"status": "unknown", "available": False}
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Test L2 (Redis)
        try:
            test_key = f"health_check_{int(time.time())}"
            await self.l2_cache.set(test_key, "test", 10)
            result = await self.l2_cache.get(test_key)
            await self.l2_cache.delete(test_key)
            
            if result == "test":
                health["levels"]["l2"]["status"] = "healthy"
                health["levels"]["l2"]["available"] = True
            else:
                health["levels"]["l2"]["status"] = "degraded"
                
        except Exception as e:
            health["levels"]["l2"]["status"] = "unhealthy"
            health["levels"]["l2"]["error"] = str(e)
        
        # Statut global
        if health["levels"]["l2"]["status"] != "healthy":
            health["status"] = "degraded"
        
        return health

# Instance globale du cache
cache = None

def initialize_cache(config: CacheConfig) -> MultiLevelCache:
    """Initialise le cache global"""
    global cache
    cache = MultiLevelCache(config)
    return cache

def get_cache() -> Optional[MultiLevelCache]:
    """Retourne l'instance globale du cache"""
    return cache
```

**Crit√®res d'acceptation :**
- Profiling complet r√©alis√©
- Goulots d'√©tranglement identifi√©s
- Cache multi-niveaux optimis√©
- Performances am√©lior√©es de 30%

#### Jour 3-4 : Infrastructure Production
**T√¢ches :**
- [ ] Images Docker optimis√©es
- [ ] Configuration Kubernetes
- [ ] Syst√®me de backup
- [ ] Monitoring complet

#### Jour 5 : Tests de Performance
**T√¢ches :**
- [ ] Tests de charge
- [ ] Tests de stress
- [ ] Benchmarks de performance
- [ ] Validation des optimisations

### Semaine 2 : S√©curit√© et Documentation

#### Jour 6-7 : S√©curit√© Renforc√©e
**T√¢ches :**
- [ ] Authentification JWT/OAuth2
- [ ] Chiffrement des donn√©es
- [ ] Audit et logging s√©curis√©
- [ ] Scan de vuln√©rabilit√©s

#### Jour 8-9 : Documentation Compl√®te
**T√¢ches :**
- [ ] Guide d'installation production
- [ ] Documentation API
- [ ] Runbooks de maintenance
- [ ] Guide de troubleshooting

#### Jour 10 : Finalisation et D√©ploiement
**T√¢ches :**
- [ ] Tests finaux end-to-end
- [ ] D√©ploiement en production
- [ ] Validation post-d√©ploiement
- [ ] Formation √©quipe

## üîß Configuration Technique

### Configuration Docker Production
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  ai-log-analyst:
    build:
      context: .
      dockerfile: Dockerfile.prod
    ports:
      - "8000:8000"
    environment:
      - ENV=production
      - LOG_LEVEL=INFO
      - REDIS_URL=redis://redis:6379
      - QDRANT_URL=http://qdrant:6333
    volumes:
      - ./logs:/app/logs
      - ./config:/app/config
    depends_on:
      - redis
      - qdrant
      - postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped

  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=ai_log_analyst
      - POSTGRES_USER=app_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - ai-log-analyst
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    restart: unless-stopped

volumes:
  redis_data:
  qdrant_data:
  postgres_data:
  prometheus_data:
  grafana_data:
```

### Configuration S√©curit√©
```yaml
# config/security.yaml
security:
  authentication:
    jwt:
      secret_key: ${JWT_SECRET_KEY}
      algorithm: "HS256"
      access_token_expire_minutes: 30
      refresh_token_expire_days: 7
    
    oauth2:
      enabled: true
      providers:
        google:
          client_id: ${GOOGLE_CLIENT_ID}
          client_secret: ${GOOGLE_CLIENT_SECRET}
        microsoft:
          client_id: ${MICROSOFT_CLIENT_ID}
          client_secret: ${MICROSOFT_CLIENT_SECRET}
  
  authorization:
    rbac_enabled: true
    roles:
      admin:
        permissions:
          - "logs:read"
          - "logs:write"
          - "config:read"
          - "config:write"
          - "users:manage"
      analyst:
        permissions:
          - "logs:read"
          - "config:read"
      viewer:
        permissions:
          - "logs:read"
  
  encryption:
    data_at_rest:
      enabled: true
      algorithm: "AES-256-GCM"
      key_rotation_days: 90
    
    data_in_transit:
      tls_version: "1.3"
      cipher_suites:
        - "TLS_AES_256_GCM_SHA384"
        - "TLS_CHACHA20_POLY1305_SHA256"
  
  audit:
    enabled: true
    log_level: "INFO"
    retention_days: 365
    events:
      - "authentication"
      - "authorization"
      - "data_access"
      - "configuration_changes"
  
  rate_limiting:
    enabled: true
    requests_per_minute: 100
    burst_size: 20
    
  cors:
    enabled: true
    allowed_origins:
      - "https://yourdomain.com"
    allowed_methods:
      - "GET"
      - "POST"
      - "PUT"
      - "DELETE"
    allowed_headers:
      - "Authorization"
      - "Content-Type"
```

## üìä M√©triques de Succ√®s

### M√©triques Performance
- **Temps r√©ponse API** : < 200ms (95e percentile)
- **D√©bit** : > 1000 req/sec
- **Utilisation CPU** : < 70%
- **Utilisation m√©moire** : < 80%

### M√©triques S√©curit√©
- **Vuln√©rabilit√©s critiques** : 0
- **Temps de d√©tection intrusion** : < 5 minutes
- **Conformit√© audit** : 100%
- **Chiffrement** : 100% des donn√©es

### M√©triques Disponibilit√©
- **Uptime** : > 99.9%
- **MTTR** : < 15 minutes
- **Backup success rate** : 100%
- **Monitoring coverage** : 100%

## üö® Risques et Mitigation

### Risques Techniques
| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|---------|------------|
| Performance d√©grad√©e | Moyenne | √âlev√© | Tests de charge + monitoring |
| Faille de s√©curit√© | Faible | Critique | Audit + scan automatis√© |
| Perte de donn√©es | Faible | Critique | Backup + r√©plication |

### Risques Op√©rationnels
| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|---------|------------|
| Complexit√© d√©ploiement | √âlev√©e | Moyen | Automatisation + documentation |
| Formation √©quipe | Moyenne | Moyen | Documentation + sessions |

## üìù Checklist de Fin de Sprint

### Optimisation Performance
- [ ] Profiling complet r√©alis√©
- [ ] Goulots d'√©tranglement optimis√©s
- [ ] Cache multi-niveaux impl√©ment√©
- [ ] Tests de performance valid√©s

### Infrastructure Production
- [ ] Images Docker optimis√©es
- [ ] Configuration Kubernetes/Docker Compose
- [ ] Syst√®me de backup automatis√©
- [ ] Monitoring et alerting complets

### S√©curit√©
- [ ] Authentification JWT/OAuth2 impl√©ment√©e
- [ ] Chiffrement des donn√©es configur√©
- [ ] Audit et logging s√©curis√©
- [ ] Scan de vuln√©rabilit√©s r√©alis√©

### Documentation
- [ ] Guide d'installation production
- [ ] Documentation API compl√®te
- [ ] Runbooks de maintenance
- [ ] Guide de troubleshooting

### D√©ploiement
- [ ] Tests finaux end-to-end r√©ussis
- [ ] D√©ploiement en production valid√©
- [ ] Monitoring post-d√©ploiement
- [ ] Formation √©quipe r√©alis√©e

## üéØ Crit√®res de Succ√®s Final

Pour consid√©rer le projet comme termin√© avec succ√®s :

1. **Performance** : Toutes les m√©triques cibles atteintes
2. **S√©curit√©** : Audit de s√©curit√© valid√© sans faille critique
3. **Disponibilit√©** : Syst√®me stable en production > 99.9%
4. **Documentation** : Documentation compl√®te et √† jour
5. **√âquipe** : Formation r√©alis√©e et autonomie op√©rationnelle

---

**üöÄ Objectif : Syst√®me optimis√©, s√©curis√© et pr√™t pour la production**

*Derni√®re mise √† jour : 07/01/2025*